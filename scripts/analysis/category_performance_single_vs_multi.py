"""
Separate category performance analysis for single-label vs multi-label
Uses existing consolidated API results
"""
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Error category mapping
error_categories = {
    'A': 'Loop Condition',
    'B': 'Condition Branch',
    'C': 'Statement Integrity',
    'D': 'I/O Format',
    'E': 'Variable Init',
    'F': 'Data Type',
    'G': 'Computation',
    'NONE': 'No Error'
}

print("="*80)
print("ðŸ“Š LOADING EXISTING API DATA")
print("="*80)

# Load existing per-category accuracy data
per_cat = pd.read_csv("data/exports/api_per_category_accuracy.csv")

# Check if there's a label_type or similar column
print("Available columns:", per_cat.columns.tolist())
print("\nFirst few rows:")
print(per_cat.head())

# We need to separate single vs multi based on the original data
# Load the comparison file if it exists
try:
    single_multi_comp = pd.read_csv("data/exports/api_single_vs_multi_comparison.csv")
    print("\nâœ“ Found single vs multi comparison file")
    print(single_multi_comp.head())
except:
    print("\nâš  No single vs multi comparison found, using alternative approach")
    
    # Alternative: Manually separate based on category presence
    # NONE only appears in single-label
    # We'll split the data accordingly
    
    per_cat['Category_Name'] = per_cat['Category'].map(error_categories)
    
    # For simplicity, let's just analyze what we have
    print("\n" + "="*80)
    print("ðŸ“Š OVERALL CATEGORY PERFORMANCE (Combined Single + Multi)")
    print("="*80)
    
    category_avg = per_cat.groupby(['Category', 'Category_Name']).agg({
        'Accuracy_%': 'mean',
        'Questions': 'sum'
    }).sort_values('Accuracy_%', ascending=False)
    
    print(category_avg.round(2).to_string())
    
    # Separate NONE from others
    print("\n" + "="*80)
    print("ðŸ’¡ KEY OBSERVATION")
    print("="*80)
    print("""
    NONE appears in the results, which suggests:
    - This data includes SINGLE-LABEL samples (where NONE is possible)
    - For true multi-label partial overlap analysis, we should exclude NONE
    
    The api_per_category_accuracy.csv combines both single and multi-label.
    To separate them properly, we need the raw prediction files.
    """)
    
    # Create a filtered version without NONE
    multi_only = per_cat[per_cat['Category'] != 'NONE'].copy()
    
    print("\n" + "="*80)
    print("ðŸ“Š MULTI-LABEL CATEGORIES (Excluding NONE)")
    print("="*80)
    
    multi_category_avg = multi_only.groupby(['Category', 'Category_Name']).agg({
        'Accuracy_%': 'mean',
        'Questions': 'sum'
    }).sort_values('Accuracy_%', ascending=False)
    
    print(multi_category_avg.round(2).to_string())
    
    # Visualization comparing with/without NONE
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Top-left: All categories (including NONE)
    cat_avg_all = per_cat.groupby('Category')['Accuracy_%'].mean().sort_values()
    colors_all = ['#d62728' if x < 30 else '#ff7f0e' if x < 40 else '#2ca02c' for x in cat_avg_all.values]
    
    axes[0,0].barh(cat_avg_all.index, cat_avg_all.values, color=colors_all, alpha=0.8)
    axes[0,0].set_xlabel('Accuracy (%)', fontweight='bold')
    axes[0,0].set_title('ALL Categories (Single + Multi)', fontsize=14, fontweight='bold')
    axes[0,0].set_xlim(0, 50)
    axes[0,0].grid(axis='x', alpha=0.3)
    
    for i, v in enumerate(cat_avg_all.values):
        axes[0,0].text(v + 0.5, i, f'{v:.1f}%', va='center', fontweight='bold')
    
    # Top-right: Multi-label only (no NONE)
    cat_avg_multi = multi_only.groupby('Category')['Accuracy_%'].mean().sort_values()
    colors_multi = ['#d62728' if x < 30 else '#ff7f0e' if x < 40 else '#2ca02c' for x in cat_avg_multi.values]
    
    axes[0,1].barh(cat_avg_multi.index, cat_avg_multi.values, color=colors_multi, alpha=0.8)
    axes[0,1].set_xlabel('Accuracy (%)', fontweight='bold')
    axes[0,1].set_title('MULTI-LABEL Only (Excluding NONE)', fontsize=14, fontweight='bold')
    axes[0,1].set_xlim(0, 50)
    axes[0,1].grid(axis='x', alpha=0.3)
    
    for i, v in enumerate(cat_avg_multi.values):
        axes[0,1].text(v + 0.5, i, f'{v:.1f}%', va='center', fontweight='bold')
    
    # Bottom-left: Heatmap - All categories
    pivot_all = per_cat.pivot_table(
        values='Accuracy_%',
        index='Model',
        columns='Category',
        aggfunc='mean'
    )
    
    sns.heatmap(pivot_all, annot=True, fmt='.1f', cmap='RdYlGn',
                vmin=0, vmax=100, ax=axes[1,0], cbar_kws={'label': 'Accuracy %'})
    axes[1,0].set_title('All Categories Heatmap', fontsize=14, fontweight='bold')
    axes[1,0].set_xlabel('')
    
    # Bottom-right: Heatmap - Multi-label only
    pivot_multi = multi_only.pivot_table(
        values='Accuracy_%',
        index='Model',
        columns='Category',
        aggfunc='mean'
    )
    
    sns.heatmap(pivot_multi, annot=True, fmt='.1f', cmap='RdYlGn',
                vmin=0, vmax=100, ax=axes[1,1], cbar_kws={'label': 'Accuracy %'})
    axes[1,1].set_title('Multi-Label Categories Heatmap', fontsize=14, fontweight='bold')
    axes[1,1].set_xlabel('')
    
    plt.tight_layout()
    plt.savefig('data/exports/category_with_without_none.png', dpi=300, bbox_inches='tight')
    print("\nâœ“ Saved: category_with_without_none.png")
    plt.show()
    
    # Summary
    print("\n" + "="*80)
    print("ðŸ“Š SUMMARY")
    print("="*80)
    
    print("\nðŸ† BEST PERFORMING CATEGORIES (Multi-Label Only):")
    for i, (cat, row) in enumerate(multi_category_avg.head(3).iterrows(), 1):
        cat_label, cat_name = cat
        print(f"   {i}. {cat_label} - {cat_name:20s}: {row['Accuracy_%']:.1f}%")
    
    print("\nâš ï¸  WORST PERFORMING CATEGORIES (Multi-Label Only):")
    for i, (cat, row) in enumerate(multi_category_avg.tail(3).iterrows(), 1):
        cat_label, cat_name = cat
        print(f"   {i}. {cat_label} - {cat_name:20s}: {row['Accuracy_%']:.1f}%")
    
    print("\nðŸ’¡ KEY INSIGHT:")
    print(f"""
    When we exclude NONE (single-label cases), we focus on actual ERROR categories.
    
    Multi-label error detection shows:
    - Best category: {cat_avg_multi.idxmax()} at {cat_avg_multi.max():.1f}%
    - Worst category: {cat_avg_multi.idxmin()} at {cat_avg_multi.min():.1f}%
    - Performance range: {(cat_avg_multi.max() - cat_avg_multi.min()):.1f}% difference
    
    This shows models vary significantly across error types, even in multi-label scenarios.
    """)
    
    # Save summary
    summary = f"""
CATEGORY PERFORMANCE ANALYSIS (With/Without NONE)
=================================================

ALL CATEGORIES (Including NONE):
{category_avg.to_string()}

MULTI-LABEL CATEGORIES ONLY (Excluding NONE):
{multi_category_avg.to_string()}

OBSERVATION:
- NONE category suggests single-label samples are included
- For multi-label partial overlap analysis, NONE is excluded
- Performance drops when NONE is removed (focusing on harder error cases)

Best Multi-Label Category: {cat_avg_multi.idxmax()} ({cat_avg_multi.max():.1f}%)
Worst Multi-Label Category: {cat_avg_multi.idxmin()} ({cat_avg_multi.min():.1f}%)
"""
    
    with open("data/exports/category_with_without_none_summary.txt", "w", encoding='utf-8') as f:
        f.write(summary)
    
    print("\nâœ“ Saved: category_with_without_none_summary.txt")
    print("="*80)
