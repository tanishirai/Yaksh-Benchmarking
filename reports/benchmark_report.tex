\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{float}
\usepackage{array}
\usepackage{enumitem}

\title{\textbf{Benchmarking Large Language Models} \\ 
       \large for Python Code Error Detection \\
       \normalsize A Comparative Study of Six AI Models}
\author{FOSSEE Internship Project \\ Bhopal, Madhya Pradesh}
\date{February 2026}

\begin{document}

\maketitle

\begin{abstract}
This report evaluates the performance of six Large Language Models (LLMs) in detecting logical errors in Python code. A dataset of 100 programming questions was manually labeled with error categories, then analyzed by each model using two prompting strategies across two independent runs. Results show significant variation in model accuracy (15-40\%), category-specific performance differences, and modest consistency across repeated evaluations. The study provides insights for selecting appropriate models for automated code review applications.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Research Context}

Automated detection of programming errors has become increasingly important for educational platforms and development tools. While Large Language Models demonstrate strong capabilities in code generation, their effectiveness at identifying specific error categories remains an open research question. This study systematically evaluates six state-of-the-art LLMs on error detection tasks.

\subsection{Research Objectives}

This benchmarking study aims to address the following questions:

\begin{enumerate}[leftmargin=*]
    \item Which LLM achieves the highest accuracy in error classification?
    \item How does performance vary across different error categories?
    \item Does prompting strategy (single vs multi-label) affect accuracy?
    \item How consistent are model predictions across independent runs?
    \item Can ensemble methods improve upon individual model performance?
\end{enumerate}

\subsection{Models Evaluated}

Six leading LLMs were selected for evaluation:

\textbf{Proprietary Models:}
\begin{itemize}[leftmargin=*]
    \item \textbf{Anthropic Claude Sonnet 4.5} - Advanced reasoning model
    \item \textbf{Google Gemini 2.5 Flash} - Optimized for speed and efficiency
    \item \textbf{OpenAI GPT-5.2} - Latest general-purpose architecture
\end{itemize}

\textbf{Open-Source Models:}
\begin{itemize}[leftmargin=*]
    \item \textbf{DeepSeek v3.2} - Chinese open-source coding model
    \item \textbf{Qwen3 Coder} - Alibaba's coding-specialized architecture
    \item \textbf{OpenAI GPT-OSS-120B} - Open-weights GPT variant
\end{itemize}

\subsection{Experimental Methodology}

\textbf{Dataset:} 100 Python programming questions from the Yaksh platform (IIT Bombay)

\textbf{Manual Annotation:} Each question was independently reviewed and labeled with applicable error categories by domain experts

\textbf{Testing Protocol:}
\begin{itemize}[leftmargin=*]
    \item Two independent runs per model (Run 1 and Run 2)
    \item Two prompting strategies: Single-label and Multi-label classification
    \item Total test configurations: 24 (6 models × 2 runs × 2 strategies)
\end{itemize}

\subsection{Error Category Definitions}

Questions were classified according to the following error taxonomy:

\begin{description}[leftmargin=*]
    \item[A - Loop Condition] Incorrect loops in for/while condition statements
    \item[B - Condition Branch] Incorrect expression in the if condition
    \item[C - Statement Integrity] Statement lacks a part of logical structure
    \item[D - Output/Input Format] Incorrect cin/cout or input/output statement
    \item[E - Variable Initialization] Incorrect declaration of variables
    \item[F - Data Type] Incorrect data type usage
    \item[G - Computation] Incorrect basic math symbols or operators
    \item[NONE] No error present (code is correct)
\end{description}

Note that questions may contain multiple error types simultaneously.

\newpage

\section{Dataset Characteristics}

\subsection{Manual Annotation Results}

Prior to model evaluation, all 100 questions underwent manual review to establish ground truth labels. This section presents the distribution and characteristics of the labeled dataset.

\subsection{Error Category Distribution}

Table 1 shows the frequency distribution of error categories across the dataset:

\begin{table}[H]
\centering
\caption{Error Category Distribution in Manual Labels}
\begin{tabular}{lcc}
\toprule
\textbf{Category} & \textbf{Count} & \textbf{Percentage} \\
\midrule
C & 51 & 51.0\% \\
D & 49 & 49.0\% \\
G & 31 & 31.0\% \\
NONE & 20 & 20.0\% \\
F & 10 & 10.0\% \\
B & 8 & 8.0\% \\
A & 5 & 5.0\% \\
E & 5 & 5.0\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Dataset Characteristics:}

\begin{itemize}[leftmargin=*]
    \item \textbf{Total category assignments: 179} - This exceeds 100 due to multi-label questions containing multiple error types
    
    \item \textbf{Most frequent: C} - Appears in 51 questions (51.0%)
    
    \item \textbf{Least frequent: E} - Appears in 5 questions (5.0%)
    
    \item \textbf{Average labels per question: 1.79} - Indicates moderate label complexity
\end{itemize}

The distribution shows reasonable category diversity, though some error types are more prevalent than others in the test set.

\newpage
\section{Model Performance Analysis}

\subsection{Overall Accuracy Rankings}

Table 2 presents the aggregate performance of each model, averaged across all runs and prompting strategies:

\begin{table}[H]
\centering
\caption{Overall Model Performance Rankings}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Accuracy (\%)} & \textbf{Rank} \\
\midrule
openai-gpt-5.2 & 40.5 & 1 \\
openai-gpt-oss-120b & 40.0 & 2 \\
anthropic-claude-sonnet-4.5 & 35.0 & 3 \\
google-gemini-2.5-flash & 33.2 & 4 \\
qwen-qwen3-coder & 23.5 & 5 \\
deepseek-deepseek-v3.2 & 16.8 & 6 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Performance Summary:}

\begin{itemize}[leftmargin=*]
    \item \textbf{Best performer: openai-gpt-5.2} achieved 40.5\% accuracy, correctly classifying approximately 40 out of 100 questions
    
    \item \textbf{Lowest performer: deepseek-deepseek-v3.2} achieved 16.8\% accuracy
    
    \item \textbf{Mean accuracy: 31.5\%} - Average performance across all models
    
    \item \textbf{Performance range: 23.8\%} - Indicates substantial variation in model capabilities
\end{itemize}

\textbf{Implications:} Current accuracy levels suggest these models are suitable for assistive roles rather than autonomous decision-making in code review contexts. Human verification remains necessary for production applications.

\newpage
\section{Category-Specific Performance}

\subsection{Per-Category Accuracy Analysis}

Model performance varies significantly across error categories. Table 3 presents accuracy breakdowns by error type:

\begin{table}[H]
\centering
\caption{Model Accuracy by Error Category (\%)}
\begin{tabular}{lcccccc}
\toprule
\textbf{Model} & \textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} & \textbf{E} & \textbf{F} \\
\midrule
anthropic-claude-sonnet-4.5 & 45 & 41 & 40 & 32 & 30 & 38 \\
deepseek-deepseek-v3.2 & 25 & 38 & 18 & 11 & 20 & 18 \\
google-gemini-2.5 & 40 & 50 & 37 & 30 & 20 & 52 \\
openai-gpt-5.2 & 40 & 38 & 47 & 46 & 50 & 48 \\
openai-gpt-oss-120b & 45 & 41 & 46 & 40 & 35 & 42 \\
qwen-qwen3-coder & 50 & 50 & 28 & 17 & 25 & 28 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Category-Specific Findings:}

\begin{itemize}[leftmargin=*]
    \item \textbf{Highest accuracy category: B} - Average 43\% across all models
    
    \item \textbf{Lowest accuracy category: D} - Average 29\% across all models
    
    \item \textbf{Standard deviation: 5.1\%} - Indicates high variability in category difficulty
\end{itemize}

\textbf{Interpretation:} Performance variability suggests that overall accuracy metrics alone are insufficient for model selection. Category-specific requirements should inform model choice for domain-specific applications.

\newpage
\section{Prompting Strategy Comparison}

\subsection{Single-Label vs Multi-Label Classification}

Two prompting strategies were evaluated:
\begin{itemize}[leftmargin=*]
    \item \textbf{Single-label:} Models select one primary error category
    \item \textbf{Multi-label:} Models select all applicable error categories
\end{itemize}

Table 4 compares performance across these strategies:

\begin{table}[H]
\centering
\caption{Single vs Multi-Label Prompting Performance}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Single (\%)} & \textbf{Multi (\%)} & \textbf{Difference (\%)} \\
\midrule
deepseek-deepseek-v3 & 29.5 & 4.0 & -25.5 \\
google-gemini-2.5-fl & 53.5 & 13.0 & -40.5 \\
qwen-qwen3-coder & 45.5 & 1.5 & -44.0 \\
anthropic-claude-son & 57.5 & 12.5 & -45.0 \\
openai-gpt-oss-120b & 65.5 & 14.5 & -51.0 \\
openai-gpt-5.2 & 69.5 & 11.5 & -58.0 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Strategy Analysis:}

\begin{itemize}[leftmargin=*]
    \item \textbf{Average difference: -44.0\%} - Multi-label prompting shows negative effect on average
    
    \item \textbf{Improved models: 0 out of 6} - Effect is not uniform across architectures
    
    \item \textbf{Best improvement: -25.5\%} (deepseek-deepseek-v3.2)
    
    \item \textbf{Largest decline: -58.0\%} (openai-gpt-5.2)
\end{itemize}

\textbf{Recommendation:} Prompting strategy should be evaluated on a per-model basis for specific deployment scenarios.

\newpage
\section{Consistency and Reliability}

\subsection{Run-to-Run Agreement Analysis}

Model consistency was evaluated by comparing predictions across two independent runs on identical inputs. Table 5 presents agreement rates:

\begin{table}[H]
\centering
\caption{Inter-Run Agreement Rates}
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{Agreement Rate (\%)} \\
\midrule
openai-gpt-5.2 & 75.5 \\
openai-gpt-oss-120b & 64.5 \\
qwen-qwen3-coder & 63.5 \\
anthropic-claude-son & 55.0 \\
google-gemini-2.5-fl & 55.0 \\
deepseek-deepseek-v3 & 44.5 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Consistency Metrics:}

\begin{itemize}[leftmargin=*]
    \item \textbf{Highest consistency: openai-gpt-5.2} - 75.5\% agreement rate
    
    \item \textbf{Lowest consistency: deepseek-deepseek-v3.2} - 44.5\% agreement rate
    
    \item \textbf{Average consistency: 59.7\%} - Mean agreement across all models
\end{itemize}

\textbf{Production Considerations:} High consistency is critical for deployment reliability. Models with lower agreement rates may require ensemble methods or human verification workflows.

\newpage
\section{Proprietary vs Open-Source Comparison}

\subsection{Cost-Performance Analysis}

Table 6 compares the average performance of proprietary (closed-source) versus open-source models:

\begin{table}[H]
\centering
\caption{Proprietary vs Open-Source Performance}
\begin{tabular}{lccc}
\toprule
\textbf{Mode} & \textbf{Proprietary (\%)} & \textbf{Open-Source (\%)} & \textbf{Gap (\%)} \\
\midrule
Single & 61.0 & 45.7 & +15.3 \\
Multi & 11.0 & 6.3 & +4.7 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Economic Analysis:}

\begin{itemize}[leftmargin=*]
    \item \textbf{Performance advantage: 10.0\%} - Proprietary models lead on average
    
    \item Cost considerations must be weighed against the observed performance differential
    
    \item Open-source models provide viable alternatives for budget-constrained deployments
\end{itemize}

\newpage
\section{Ensemble Methods}

\subsection{Majority Voting Analysis}

An ensemble approach using majority voting across all six models was evaluated. Table 7 shows results:

\begin{table}[H]
\centering
\caption{Ensemble Performance vs Individual Models}
\begin{tabular}{lccc}
\toprule
\textbf{Mode} & \textbf{Avg Individual (\%)} & \textbf{Ensemble (\%)} & \textbf{Improvement (\%)} \\
\midrule
Single & 53.3 & 57.0 & +3.7 \\
Multi & 8.7 & 11.0 & +2.3 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Ensemble Insights:}

\begin{itemize}[leftmargin=*]
    \item Majority voting demonstrates improved accuracy over average individual performance
    \item Computational cost increases linearly with ensemble size
    \item Suitable for high-criticality applications where accuracy justifies additional overhead
\end{itemize}

\newpage
\section{Conclusions and Recommendations}

\subsection{Key Findings}

\begin{enumerate}[leftmargin=*]
    \item \textbf{Performance Range:} Model accuracy ranges from 15-40\%, indicating substantial room for improvement in error detection capabilities.
    
    \item \textbf{Category Dependence:} Performance varies significantly by error type, with some categories proving systematically more challenging than others.
    
    \item \textbf{Prompting Effects:} Multi-label prompting shows inconsistent effects across models, requiring case-by-case evaluation.
    
    \item \textbf{Consistency Variation:} Inter-run agreement rates vary substantially, with implications for production reliability.
    
    \item \textbf{Commercial Advantage:} Proprietary models demonstrate modest performance gains over open-source alternatives.
    
    \item \textbf{Ensemble Benefits:} Majority voting improves accuracy but increases computational requirements.
\end{enumerate}

\subsection{Deployment Recommendations}

\textbf{Educational Applications:}
\begin{itemize}[leftmargin=*]
    \item Deploy models as supplementary learning aids rather than primary assessment tools
    \item Emphasize category-specific strengths when providing automated feedback
    \item Maintain human oversight for final grading decisions
\end{itemize}

\textbf{Code Review Integration:}
\begin{itemize}[leftmargin=*]
    \item Implement ensemble methods for critical code paths
    \item Establish category-specific confidence thresholds
    \item Flag low-consensus predictions for manual review
\end{itemize}

\textbf{Model Selection Criteria:}
\begin{itemize}[leftmargin=*]
    \item Prioritize consistency metrics alongside accuracy
    \item Evaluate performance on domain-specific error categories
    \item Consider open-source alternatives for cost-sensitive applications
\end{itemize}

\subsection{Study Limitations}

\begin{itemize}[leftmargin=*]
    \item Sample size limited to 100 questions (larger datasets recommended)
    \item Single programming language scope (Python only)
    \item Binary evaluation metric (correct/incorrect without partial credit)
    \item Limited to released model versions (subject to rapid obsolescence)
\end{itemize}

\subsection{Future Research Directions}

\begin{itemize}[leftmargin=*]
    \item Expand dataset to 500+ questions for improved statistical power
    \item Evaluate domain-specific fine-tuned models
    \item Investigate advanced prompting techniques (chain-of-thought, few-shot learning)
    \item Extend analysis to additional programming languages
    \item Assess explanation quality alongside classification accuracy
    \item Track performance evolution as models are updated
\end{itemize}

\subsection{Concluding Remarks}

Current LLM performance in code error detection demonstrates promise but remains insufficient for autonomous deployment. The observed accuracy ceiling around 40\% suggests fundamental limitations in semantic code understanding. However, strategic deployment through ensemble methods, category-specific application, and human-in-the-loop workflows can enable practical value extraction from these technologies.

Given rapid advancement in the field, periodic re-evaluation is recommended to track performance improvements in next-generation models.

\end{document}
